# Sp√©cification : Syst√®me de Rattrapage de Donn√©es (Data Backfill)

## Vue d'ensemble

Syst√®me permettant de combler les lacunes de donn√©es historiques pour les tokens existants, en r√©utilisant l'infrastructure d'initialisation historique existante.

**Date** : 2025-10-24
**Version** : 1.0
**Statut** : üìã Conception (√† impl√©menter)

---

## 1. Cas d'usage

### 1.1 Sc√©narios n√©cessitant un rattrapage

| Sc√©nario | Description | Priorit√© |
|----------|-------------|----------|
| **Panne syst√®me** | L'API √©tait arr√™t√©e pendant plusieurs heures/jours | üî¥ Haute |
| **Erreur de collecte** | Des donn√©es manquantes dues √† des erreurs API | üü† Moyenne |
| **Token ajout√© avant la feature** | Tokens existants sans historique (status 'skipped') | üü° Moyenne |
| **Extension d'historique** | Ajouter plus de 30 jours d'historique si disponible | üü¢ Basse |
| **Donn√©es corrompues** | Besoin de recharger une p√©riode sp√©cifique | üü† Moyenne |

### 1.2 Exemple concret

**Situation** : L'API √©tait en panne du 20 au 22 octobre (2 jours)

**Sans rattrapage** :
```
15 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
16 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
17 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
18 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
19 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
20 oct. ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚Üê Lacune
21 oct. ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚Üê Lacune
22 oct. ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚Üê Lacune
23 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
24 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
```

**Avec rattrapage** :
```
15 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
16 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
17 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
18 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
19 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
20 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚Üê Rattrap√© via GeckoTerminal
21 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚Üê Rattrap√© via GeckoTerminal
22 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚Üê Rattrap√© via GeckoTerminal
23 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
24 oct. ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
```

---

## 2. Architecture propos√©e

### 2.1 R√©utilisation de l'existant

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              GeckoTerminalClient (existant)                  ‚îÇ
‚îÇ  - fetchOHLCVHistory(poolId, daysBack, onProgress)          ‚îÇ
‚îÇ  - getMainPoolId(tokenAddress)                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         HistoricalDataInitializer (existant)                 ‚îÇ
‚îÇ  - storeHistoricalData(token, candles)                      ‚îÇ
‚îÇ  - buildOHLCVCandles(token, rawCandles)                     ‚îÇ
‚îÇ  - aggregateCandles(minuteCandles, timeframe)               ‚îÇ
‚îÇ  - calculateRSI(candles)                                    ‚îÇ
‚îÇ  - calculateEMA(prices)                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           DataBackfillService (NOUVEAU)                      ‚îÇ
‚îÇ  - detectGaps(tokenAddress, startDate, endDate)             ‚îÇ
‚îÇ  - backfillToken(tokenAddress, startDate, endDate)          ‚îÇ
‚îÇ  - backfillAllTokens(startDate, endDate)                    ‚îÇ
‚îÇ  - getBackfillStatus(tokenAddress)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Nouvelle table SQLite

```sql
-- Table pour tracker les op√©rations de rattrapage
CREATE TABLE IF NOT EXISTS backfill_jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    token_address TEXT NOT NULL,
    start_date INTEGER NOT NULL,      -- Timestamp de d√©but de la p√©riode √† rattraper
    end_date INTEGER NOT NULL,        -- Timestamp de fin de la p√©riode √† rattraper
    status TEXT DEFAULT 'pending',    -- 'pending', 'in_progress', 'completed', 'failed'
    created_at INTEGER DEFAULT (strftime('%s', 'now')),
    started_at INTEGER,
    completed_at INTEGER,
    progress INTEGER DEFAULT 0,
    candles_filled INTEGER DEFAULT 0,
    error_message TEXT,
    FOREIGN KEY (token_address) REFERENCES tokens(contract_address)
);

CREATE INDEX idx_backfill_status ON backfill_jobs(status);
CREATE INDEX idx_backfill_token ON backfill_jobs(token_address);
```

---

## 3. D√©tection des lacunes

### 3.1 Algorithme de d√©tection

```javascript
async function detectGaps(tokenAddress, startDate, endDate) {
    // 1. Requ√™te InfluxDB pour obtenir toutes les bougies 1m de la p√©riode
    const query = `
        from(bucket: "${bucket}")
        |> range(start: ${startDate}, stop: ${endDate})
        |> filter(fn: (r) => r["_measurement"] == "ohlcv")
        |> filter(fn: (r) => r.contract_address == "${tokenAddress}")
        |> filter(fn: (r) => r.timeframe == "1m")
        |> keep(columns: ["_time"])
    `;

    const existingCandles = await queryInflux(query);

    // 2. G√©n√©rer la liste compl√®te des timestamps attendus (1 minute = 60s)
    const expectedTimestamps = [];
    for (let ts = startDate; ts < endDate; ts += 60) {
        expectedTimestamps.push(ts);
    }

    // 3. Identifier les timestamps manquants
    const existingTimestamps = new Set(existingCandles.map(c => c._time));
    const gaps = expectedTimestamps.filter(ts => !existingTimestamps.has(ts));

    // 4. Regrouper les lacunes cons√©cutives en plages
    const gapRanges = groupConsecutiveGaps(gaps);

    return gapRanges; // [{start: timestamp, end: timestamp, duration: minutes}]
}

function groupConsecutiveGaps(gaps) {
    if (gaps.length === 0) return [];

    const ranges = [];
    let rangeStart = gaps[0];
    let rangeEnd = gaps[0];

    for (let i = 1; i < gaps.length; i++) {
        if (gaps[i] === rangeEnd + 60) {
            // Timestamp cons√©cutif
            rangeEnd = gaps[i];
        } else {
            // Nouvelle plage
            ranges.push({
                start: rangeStart,
                end: rangeEnd,
                duration: (rangeEnd - rangeStart) / 60 + 1
            });
            rangeStart = gaps[i];
            rangeEnd = gaps[i];
        }
    }

    // Ajouter la derni√®re plage
    ranges.push({
        start: rangeStart,
        end: rangeEnd,
        duration: (rangeEnd - rangeStart) / 60 + 1
    });

    return ranges;
}
```

### 3.2 Exemple de d√©tection

**Donn√©es existantes** :
```
10:00 ‚úÖ
10:01 ‚úÖ
10:02 ‚ùå  ‚Üê Gap
10:03 ‚ùå  ‚Üê Gap
10:04 ‚ùå  ‚Üê Gap
10:05 ‚úÖ
10:06 ‚úÖ
10:07 ‚ùå  ‚Üê Gap
10:08 ‚úÖ
```

**R√©sultat** :
```javascript
[
  { start: "10:02", end: "10:04", duration: 3 },  // Plage de 3 minutes
  { start: "10:07", end: "10:07", duration: 1 }   // Plage de 1 minute
]
```

---

## 4. Service DataBackfillService

### 4.1 Fichier : `src/services/DataBackfillService.js`

```javascript
const GeckoTerminalClient = require('../clients/GeckoTerminalClient');
const HistoricalDataInitializer = require('./HistoricalDataInitializer');
const Token = require('../models/Token');
const { queryApi } = require('../config/influxdb');
const logger = require('../config/logger');

class DataBackfillService {
    constructor() {
        this.geckoClient = new GeckoTerminalClient();
        this.historicalInit = new HistoricalDataInitializer();
        this.isProcessing = false;
        this.currentJob = null;
    }

    /**
     * D√©tecter les lacunes de donn√©es pour un token
     * @param {string} tokenAddress
     * @param {Date} startDate
     * @param {Date} endDate
     * @returns {Array} Liste des plages manquantes
     */
    async detectGaps(tokenAddress, startDate, endDate) {
        logger.info(`D√©tection des lacunes pour ${tokenAddress} entre ${startDate} et ${endDate}`);

        // Impl√©mentation de l'algorithme ci-dessus
        // ...

        return gaps;
    }

    /**
     * Rattraper les donn√©es d'un token pour une p√©riode sp√©cifique
     * @param {string} tokenAddress
     * @param {Date} startDate
     * @param {Date} endDate
     */
    async backfillToken(tokenAddress, startDate, endDate) {
        logger.info(`üîÑ Rattrapage de donn√©es: ${tokenAddress} du ${startDate} au ${endDate}`);

        const token = await Token.findByAddress(tokenAddress);
        if (!token) {
            throw new Error('Token non trouv√©');
        }

        // 1. Cr√©er un job de rattrapage
        const jobId = await this.createBackfillJob(tokenAddress, startDate, endDate);

        try {
            // 2. Marquer comme en cours
            await this.updateJobStatus(jobId, 'in_progress', { started_at: Date.now() });

            // 3. R√©cup√©rer le pool ID (depuis SQLite si existe, sinon GeckoTerminal)
            let poolId = token.main_pool_id;
            if (!poolId) {
                poolId = await this.geckoClient.getMainPoolId(tokenAddress);
                await Token.updateInitializationStatus(tokenAddress, { main_pool_id: poolId });
            }

            // 4. Calculer le nombre de jours √† r√©cup√©rer
            const daysDiff = Math.ceil((endDate - startDate) / (1000 * 60 * 60 * 24));

            // 5. R√©cup√©rer les donn√©es de GeckoTerminal
            logger.info(`R√©cup√©ration de ${daysDiff} jours de donn√©es...`);

            const onProgress = (current, total) => {
                const progress = Math.floor((current / total) * 100);
                this.updateJobStatus(jobId, 'in_progress', { progress });
            };

            // Utiliser beforeTimestamp = endDate pour r√©cup√©rer l'historique
            const candles = await this.geckoClient.fetchOHLCVHistory(
                poolId,
                daysDiff,
                onProgress
            );

            // 6. Filtrer les candles pour garder uniquement la p√©riode demand√©e
            const startTimestamp = Math.floor(startDate.getTime() / 1000);
            const endTimestamp = Math.floor(endDate.getTime() / 1000);

            const filteredCandles = candles.filter(candle => {
                const [timestamp] = candle;
                return timestamp >= startTimestamp && timestamp <= endTimestamp;
            });

            logger.info(`${filteredCandles.length} candles filtr√©es pour la p√©riode`);

            // 7. D√©tecter les donn√©es d√©j√† existantes pour √©viter les doublons
            const existingTimestamps = await this.getExistingTimestamps(
                tokenAddress,
                startDate,
                endDate
            );

            const newCandles = filteredCandles.filter(candle => {
                const [timestamp] = candle;
                return !existingTimestamps.has(timestamp);
            });

            logger.info(`${newCandles.length} nouvelles candles √† ins√©rer (${filteredCandles.length - newCandles.length} doublons √©vit√©s)`);

            // 8. Stocker les donn√©es (r√©utilisation du code existant)
            if (newCandles.length > 0) {
                await this.historicalInit.storeHistoricalData(token, newCandles);
            }

            // 9. Marquer comme termin√©
            await this.updateJobStatus(jobId, 'completed', {
                completed_at: Date.now(),
                progress: 100,
                candles_filled: newCandles.length
            });

            logger.info(`‚úÖ Rattrapage termin√©: ${newCandles.length} candles ajout√©es`);

        } catch (error) {
            logger.error(`‚ùå Erreur lors du rattrapage:`, error);
            await this.updateJobStatus(jobId, 'failed', {
                error_message: error.message
            });
            throw error;
        }
    }

    /**
     * Rattraper les donn√©es de tous les tokens actifs
     * @param {Date} startDate
     * @param {Date} endDate
     */
    async backfillAllTokens(startDate, endDate) {
        const tokens = await Token.getAllActive();
        logger.info(`Rattrapage de ${tokens.length} tokens du ${startDate} au ${endDate}`);

        for (const token of tokens) {
            try {
                await this.backfillToken(token.contract_address, startDate, endDate);
            } catch (error) {
                logger.error(`Erreur pour ${token.symbol}:`, error);
                // Continue avec le prochain token
            }
        }

        logger.info(`‚úÖ Rattrapage termin√© pour tous les tokens`);
    }

    /**
     * R√©cup√©rer les timestamps d√©j√† pr√©sents dans InfluxDB
     */
    async getExistingTimestamps(tokenAddress, startDate, endDate) {
        const query = `
            from(bucket: "${process.env.INFLUXDB_BUCKET}")
            |> range(start: ${startDate.toISOString()}, stop: ${endDate.toISOString()})
            |> filter(fn: (r) => r["_measurement"] == "raw_prices")
            |> filter(fn: (r) => r.contract_address == "${tokenAddress}")
            |> keep(columns: ["_time"])
        `;

        const results = await queryApi.collectRows(query);
        const timestamps = new Set(results.map(r => Math.floor(new Date(r._time).getTime() / 1000)));

        return timestamps;
    }

    /**
     * Cr√©er un job de rattrapage
     */
    async createBackfillJob(tokenAddress, startDate, endDate) {
        const db = require('../config/sqlite').getDb();
        const stmt = db.prepare(`
            INSERT INTO backfill_jobs (token_address, start_date, end_date, status)
            VALUES (?, ?, ?, 'pending')
        `);

        const result = stmt.run(
            tokenAddress,
            Math.floor(startDate.getTime() / 1000),
            Math.floor(endDate.getTime() / 1000)
        );

        return result.lastInsertRowid;
    }

    /**
     * Mettre √† jour le statut d'un job
     */
    async updateJobStatus(jobId, status, updates = {}) {
        const db = require('../config/sqlite').getDb();

        const fields = ['status = ?'];
        const values = [status];

        Object.entries(updates).forEach(([key, value]) => {
            fields.push(`${key} = ?`);
            values.push(value);
        });

        values.push(jobId);

        const stmt = db.prepare(`
            UPDATE backfill_jobs
            SET ${fields.join(', ')}
            WHERE id = ?
        `);

        stmt.run(...values);
    }

    /**
     * Obtenir le statut d'un job de rattrapage
     */
    async getBackfillStatus(tokenAddress) {
        const db = require('../config/sqlite').getDb();
        const stmt = db.prepare(`
            SELECT * FROM backfill_jobs
            WHERE token_address = ?
            ORDER BY created_at DESC
        `);

        return stmt.all(tokenAddress);
    }
}

module.exports = DataBackfillService;
```

---

## 5. Routes API

### 5.1 Fichier : `src/routes/backfill.js`

```javascript
const express = require('express');
const router = express.Router();
const DataBackfillService = require('../services/DataBackfillService');
const logger = require('../config/logger');

const backfillService = new DataBackfillService();

/**
 * POST /api/backfill/detect-gaps
 * D√©tecter les lacunes de donn√©es pour un token
 */
router.post('/detect-gaps', async (req, res) => {
    try {
        const { token_address, start_date, end_date } = req.body;

        const gaps = await backfillService.detectGaps(
            token_address,
            new Date(start_date),
            new Date(end_date)
        );

        res.json({
            status: 'success',
            data: {
                token_address,
                period: { start_date, end_date },
                gaps_found: gaps.length,
                gaps: gaps,
                total_missing_minutes: gaps.reduce((sum, g) => sum + g.duration, 0)
            }
        });
    } catch (error) {
        logger.error('Erreur d√©tection lacunes:', error);
        res.status(500).json({
            status: 'error',
            message: error.message
        });
    }
});

/**
 * POST /api/backfill/token
 * Rattraper les donn√©es d'un token sp√©cifique
 */
router.post('/token', async (req, res) => {
    try {
        const { token_address, start_date, end_date } = req.body;

        // Validation
        if (!token_address || !start_date || !end_date) {
            return res.status(400).json({
                status: 'error',
                message: 'Param√®tres manquants: token_address, start_date, end_date requis'
            });
        }

        // Lancer le rattrapage en arri√®re-plan
        backfillService.backfillToken(
            token_address,
            new Date(start_date),
            new Date(end_date)
        ).catch(err => logger.error('Erreur rattrapage:', err));

        res.json({
            status: 'success',
            message: 'Rattrapage d√©marr√© en arri√®re-plan',
            data: {
                token_address,
                start_date,
                end_date
            }
        });
    } catch (error) {
        logger.error('Erreur lancement rattrapage:', error);
        res.status(500).json({
            status: 'error',
            message: error.message
        });
    }
});

/**
 * POST /api/backfill/all-tokens
 * Rattraper les donn√©es de tous les tokens actifs
 */
router.post('/all-tokens', async (req, res) => {
    try {
        const { start_date, end_date } = req.body;

        if (!start_date || !end_date) {
            return res.status(400).json({
                status: 'error',
                message: 'Param√®tres manquants: start_date, end_date requis'
            });
        }

        // Lancer le rattrapage en arri√®re-plan
        backfillService.backfillAllTokens(
            new Date(start_date),
            new Date(end_date)
        ).catch(err => logger.error('Erreur rattrapage global:', err));

        res.json({
            status: 'success',
            message: 'Rattrapage global d√©marr√© en arri√®re-plan',
            data: { start_date, end_date }
        });
    } catch (error) {
        logger.error('Erreur lancement rattrapage global:', error);
        res.status(500).json({
            status: 'error',
            message: error.message
        });
    }
});

/**
 * GET /api/backfill/status/:token_address
 * Obtenir le statut des jobs de rattrapage pour un token
 */
router.get('/status/:token_address', async (req, res) => {
    try {
        const { token_address } = req.params;

        const jobs = await backfillService.getBackfillStatus(token_address);

        res.json({
            status: 'success',
            data: {
                token_address,
                jobs
            }
        });
    } catch (error) {
        logger.error('Erreur r√©cup√©ration statut:', error);
        res.status(500).json({
            status: 'error',
            message: error.message
        });
    }
});

/**
 * GET /api/backfill/jobs
 * Lister tous les jobs de rattrapage
 */
router.get('/jobs', async (req, res) => {
    try {
        const db = require('../config/sqlite').getDb();
        const stmt = db.prepare(`
            SELECT b.*, t.symbol
            FROM backfill_jobs b
            JOIN tokens t ON b.token_address = t.contract_address
            ORDER BY b.created_at DESC
            LIMIT 100
        `);

        const jobs = stmt.all();

        res.json({
            status: 'success',
            data: { jobs }
        });
    } catch (error) {
        logger.error('Erreur r√©cup√©ration jobs:', error);
        res.status(500).json({
            status: 'error',
            message: error.message
        });
    }
});

module.exports = router;
```

---

## 6. Exemples d'utilisation

### 6.1 D√©tecter les lacunes

```bash
# D√©tecter les lacunes pour FROGG entre le 20 et le 24 octobre
curl -X POST http://localhost:3002/api/backfill/detect-gaps \
  -H "Content-Type: application/json" \
  -d '{
    "token_address": "ADSXPGwP3riuvqYtwqogCD4Rfn1a6NASqaSpThpsmoon",
    "start_date": "2025-10-20T00:00:00Z",
    "end_date": "2025-10-24T00:00:00Z"
  }'

# R√©ponse :
{
  "status": "success",
  "data": {
    "token_address": "ADSXPGwP3riuvqYtwqogCD4Rfn1a6NASqaSpThpsmoon",
    "period": {
      "start_date": "2025-10-20T00:00:00Z",
      "end_date": "2025-10-24T00:00:00Z"
    },
    "gaps_found": 2,
    "gaps": [
      {
        "start": 1729411200,
        "end": 1729584000,
        "duration": 2880
      },
      {
        "start": 1729670400,
        "end": 1729670460,
        "duration": 1
      }
    ],
    "total_missing_minutes": 2881
  }
}
```

### 6.2 Rattraper un token sp√©cifique

```bash
# Rattraper les donn√©es de FROGG pour la p√©riode manquante
curl -X POST http://localhost:3002/api/backfill/token \
  -H "Content-Type: application/json" \
  -d '{
    "token_address": "ADSXPGwP3riuvqYtwqogCD4Rfn1a6NASqaSpThpsmoon",
    "start_date": "2025-10-20T00:00:00Z",
    "end_date": "2025-10-22T23:59:59Z"
  }'

# R√©ponse :
{
  "status": "success",
  "message": "Rattrapage d√©marr√© en arri√®re-plan",
  "data": {
    "token_address": "ADSXPGwP3riuvqYtwqogCD4Rfn1a6NASqaSpThpsmoon",
    "start_date": "2025-10-20T00:00:00Z",
    "end_date": "2025-10-22T23:59:59Z"
  }
}
```

### 6.3 Rattraper tous les tokens

```bash
# Rattraper tous les tokens actifs pour une panne de 2 jours
curl -X POST http://localhost:3002/api/backfill/all-tokens \
  -H "Content-Type: application/json" \
  -d '{
    "start_date": "2025-10-20T00:00:00Z",
    "end_date": "2025-10-22T23:59:59Z"
  }'

# R√©ponse :
{
  "status": "success",
  "message": "Rattrapage global d√©marr√© en arri√®re-plan",
  "data": {
    "start_date": "2025-10-20T00:00:00Z",
    "end_date": "2025-10-22T23:59:59Z"
  }
}
```

### 6.4 Suivre la progression

```bash
# Voir le statut des jobs de rattrapage pour un token
curl http://localhost:3002/api/backfill/status/ADSXPGwP3riuvqYtwqogCD4Rfn1a6NASqaSpThpsmoon

# R√©ponse :
{
  "status": "success",
  "data": {
    "token_address": "ADSXPGwP3riuvqYtwqogCD4Rfn1a6NASqaSpThpsmoon",
    "jobs": [
      {
        "id": 1,
        "token_address": "ADSXPGwP3riuvqYtwqogCD4Rfn1a6NASqaSpThpsmoon",
        "start_date": 1729382400,
        "end_date": 1729641599,
        "status": "completed",
        "created_at": 1729756800,
        "started_at": 1729756801,
        "completed_at": 1729756925,
        "progress": 100,
        "candles_filled": 2880,
        "error_message": null
      }
    ]
  }
}

# Voir tous les jobs
curl http://localhost:3002/api/backfill/jobs
```

---

## 7. Gestion des doublons

### 7.1 Strat√©gie anti-doublons

**Probl√®me** : √âviter d'ins√©rer des donn√©es d√©j√† pr√©sentes dans InfluxDB

**Solution** :
1. Avant l'insertion, requ√™ter InfluxDB pour obtenir tous les timestamps existants de la p√©riode
2. Filtrer les nouvelles candles pour exclure celles d√©j√† pr√©sentes
3. N'ins√©rer que les candles manquantes

```javascript
// Pseudo-code
const existingTimestamps = await getExistingTimestamps(token, startDate, endDate);
const newCandles = fetchedCandles.filter(c => !existingTimestamps.has(c.timestamp));
await storeHistoricalData(token, newCandles);  // Insert uniquement les nouvelles
```

### 7.2 Gestion des conflits

Si des donn√©es existent d√©j√† mais sont corrompues :

**Option 1 : Skip (d√©faut)**
```javascript
// Ignorer les timestamps existants
const newCandles = candles.filter(c => !existingTimestamps.has(c.timestamp));
```

**Option 2 : Overwrite (param√®tre optionnel)**
```javascript
// Supprimer les anciennes donn√©es et r√©ins√©rer
await deleteCandles(token, startDate, endDate);
await storeHistoricalData(token, allCandles);
```

---

## 8. Limitations et consid√©rations

### 8.1 Rate Limit GeckoTerminal

- **Limite** : 30 requ√™tes / minute
- **Impact** : Pour 30 jours d'historique = ~44 requ√™tes = ~2 minutes
- **Mitigation** : File d'attente si plusieurs tokens

### 8.2 Performance InfluxDB

- **√âcriture** : ~9000 candles en quelques secondes
- **Lecture** : D√©tection des lacunes peut √™tre lente sur de grandes p√©riodes
- **Optimisation** : Limiter la d√©tection √† des p√©riodes raisonnables (<90 jours)

### 8.3 Disponibilit√© des donn√©es

- **GeckoTerminal** : Historique limit√© (variable selon le token)
- **Token r√©cent** : Peut ne pas avoir 30 jours d'historique
- **Solution** : Utiliser `fetchOHLCVHistory` qui s'arr√™te quand plus de donn√©es

### 8.4 Coh√©rence des donn√©es

- **RSI/EMA** : Calcul√©s avec l'historique disponible
- **Quality Factor** : 1.0 pour les donn√©es de rattrapage (comme l'initialisation)
- **Gaps restants** : Certaines p√©riodes peuvent rester manquantes si GeckoTerminal n'a pas les donn√©es

---

## 9. Tests recommand√©s

### 9.1 Tests unitaires

```javascript
describe('DataBackfillService', () => {
    describe('detectGaps', () => {
        it('devrait d√©tecter une lacune de 3 minutes', async () => {
            const gaps = await service.detectGaps(token, start, end);
            expect(gaps).toHaveLength(1);
            expect(gaps[0].duration).toBe(3);
        });

        it('devrait regrouper les lacunes cons√©cutives', async () => {
            const gaps = await service.detectGaps(token, start, end);
            expect(gaps[0].start).toBe(timestamp1);
            expect(gaps[0].end).toBe(timestamp5);
        });
    });

    describe('backfillToken', () => {
        it('devrait √©viter les doublons', async () => {
            await service.backfillToken(token, start, end);
            const inserted = await getInsertedCount();
            expect(inserted).toBe(expectedNewCandles);
        });
    });
});
```

### 9.2 Tests d'int√©gration

1. **Sc√©nario panne syst√®me**
   - Arr√™ter l'API pendant 2 heures
   - Red√©marrer
   - Lancer le rattrapage
   - V√©rifier que toutes les donn√©es sont pr√©sentes

2. **Sc√©nario token existant**
   - Prendre un token avec status 'skipped'
   - Lancer le rattrapage sur 30 jours
   - V√©rifier les donn√©es historiques

---

## 10. Timeline d'impl√©mentation

### Phase 1 : Fondations (2-3h)
- ‚úÖ Cr√©er la table `backfill_jobs`
- ‚úÖ Cr√©er `DataBackfillService` de base
- ‚úÖ Impl√©menter `detectGaps`
- ‚úÖ Tests unitaires

### Phase 2 : Rattrapage simple (3-4h)
- ‚úÖ Impl√©menter `backfillToken`
- ‚úÖ Gestion des doublons
- ‚úÖ Routes API `/detect-gaps` et `/token`
- ‚úÖ Tests d'int√©gration

### Phase 3 : Rattrapage global (2-3h)
- ‚úÖ Impl√©menter `backfillAllTokens`
- ‚úÖ Queue management pour √©viter rate limit
- ‚úÖ Route `/all-tokens`
- ‚úÖ Monitoring et logs

### Phase 4 : Interface & monitoring (2-3h)
- ‚úÖ Route `/status` et `/jobs`
- ‚úÖ Documentation README
- ‚úÖ Exemples d'utilisation
- ‚úÖ Tests en production

**Total estim√© : 9-13 heures**

---

## 11. Am√©liorations futures (optionnel)

### 11.1 D√©tection automatique

Ajouter un cron job qui d√©tecte automatiquement les lacunes :

```javascript
// Tous les jours √† 3h00
cron.schedule('0 3 * * *', async () => {
    const yesterday = new Date(Date.now() - 24*60*60*1000);
    const today = new Date();

    const tokens = await Token.getAllActive();
    for (const token of tokens) {
        const gaps = await backfillService.detectGaps(token.contract_address, yesterday, today);
        if (gaps.length > 0) {
            logger.warn(`Lacunes d√©tect√©es pour ${token.symbol}`);
            await backfillService.backfillToken(token.contract_address, yesterday, today);
        }
    }
});
```

### 11.2 Notifications

- Email/Slack quand des lacunes sont d√©tect√©es
- Rapport quotidien de l'√©tat des donn√©es
- Alertes si rattrapage √©choue

### 11.3 Interface web

Dashboard pour :
- Visualiser les lacunes
- Lancer des rattrapages en un clic
- Voir l'historique des jobs

### 11.4 Modes avanc√©s

- **Mode "preview"** : D√©tecter sans rattraper
- **Mode "dry-run"** : Simuler le rattrapage
- **Mode "aggressive"** : Overwrite les donn√©es existantes

---

## 12. Conclusion

### Avantages du syst√®me propos√©

‚úÖ **R√©utilisation maximale** : 90% du code existant
‚úÖ **Simplicit√©** : API REST intuitive
‚úÖ **Flexibilit√©** : Token unique ou tous les tokens
‚úÖ **Tra√ßabilit√©** : Table `backfill_jobs` pour l'audit
‚úÖ **Robustesse** : Gestion des doublons et erreurs
‚úÖ **Performance** : Filtrage c√¥t√© client pour √©viter doublons

### Risques et mitigations

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| Rate limit d√©pass√© | Moyenne | Moyen | Queue avec d√©lais |
| Doublons dans InfluxDB | Faible | Faible | V√©rification avant insert |
| Donn√©es GeckoTerminal manquantes | √âlev√©e | Moyen | Logger et continuer |
| Performance InfluxDB | Faible | Moyen | Limiter p√©riode de d√©tection |

---

**Auteur** : Claude Code
**Date de cr√©ation** : 2025-10-24
**Derni√®re mise √† jour** : 2025-10-24
**Statut** : üìã Pr√™t pour impl√©mentation
